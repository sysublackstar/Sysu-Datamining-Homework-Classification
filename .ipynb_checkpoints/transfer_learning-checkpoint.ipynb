{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator  \n",
    "from keras.preprocessing import image\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22564 images belonging to 2 classes.\n",
      "Found 2513 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('waste-classification-data/DATASET/TRAIN',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('waste-classification-data/DATASET/TEST',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = NASNetMobile(weights='imagenet', include_top=False, pooling='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard = TensorBoard(log_dir='log_nas/')\n",
    "checkpointer = ModelCheckpoint(filepath='tmp/weights.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = base_model.output\n",
    "# x = Dense(64, activation='relu',kernel_initializer='glorot_normal')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dense(32, activation='relu',kernel_initializer='glorot_normal')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(16, activation='relu')(x)\n",
    "# predictions = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "# model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "# model.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
    "#                    metrics = ['accuracy'])\n",
    "\n",
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3),\n",
    "                      activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "model.add(Dense(units = 128, activation = 'relu'))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 105s 211ms/step - loss: 0.4535 - acc: 0.7936 - val_loss: 0.3301 - val_acc: 0.8804\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33006, saving model to tmp/weights.hdf5\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 95s 190ms/step - loss: 0.3920 - acc: 0.8253 - val_loss: 0.3034 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33006 to 0.30339, saving model to tmp/weights.hdf5\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 143s 285ms/step - loss: 0.3735 - acc: 0.8402 - val_loss: 0.2971 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.30339 to 0.29714, saving model to tmp/weights.hdf5\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 128s 256ms/step - loss: 0.3545 - acc: 0.8477 - val_loss: 0.3138 - val_acc: 0.8783\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.29714\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.3315 - acc: 0.8587 - val_loss: 0.3269 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29714\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 127s 254ms/step - loss: 0.3050 - acc: 0.8739 - val_loss: 0.2930 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29714 to 0.29300, saving model to tmp/weights.hdf5\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.2902 - acc: 0.8778 - val_loss: 0.2403 - val_acc: 0.9037\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.29300 to 0.24028, saving model to tmp/weights.hdf5\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.2756 - acc: 0.8878 - val_loss: 0.2796 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24028\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 117s 235ms/step - loss: 0.2442 - acc: 0.9018 - val_loss: 0.2846 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24028\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 113s 225ms/step - loss: 0.2479 - acc: 0.8984 - val_loss: 0.2894 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x220d7aff588>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_set,\n",
    "                         steps_per_epoch = 500,\n",
    "                         epochs = 10,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000,\n",
    "                   callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model_transfer_nasnet.h5')\n",
    "model.save('cnn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
